{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d43b99c",
   "metadata": {},
   "source": [
    "# Lesson 3: The Inference Engine\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Note:</b> Intel OpenVINO toolkit should be installed and sourced to run the given code\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14eb501e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import IFrame #to show YT videos on notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2ec8f4",
   "metadata": {},
   "source": [
    "# Excercise 1. Feed an IR to the Inference Engine\n",
    "\n",
    "Make sure to click the button below before you get started to source the correct environment.\n",
    "\n",
    "\n",
    "Earlier in the course, you were focused on working with the Intermediate Representation (IR)\n",
    "models themselves, while mostly glossing over the use of the actual Inference Engine with\n",
    "the model.\n",
    "\n",
    "Here, you'll import the Python wrapper for the Inference Engine (IE), and practice using \n",
    "different IRs with it. You will first add each IR as an `IENetwork`, and check whether the layers \n",
    "of that network are supported by the classroom CPU.\n",
    "\n",
    "Since the classroom workspace is using an Intel CPU, you will also need to add a CPU\n",
    "extension to the `IECore`.\n",
    "\n",
    "Once you have verified all layers are supported (when the CPU extension is added),\n",
    "you will load the given model into the Inference Engine.\n",
    "\n",
    "Note that the `.xml` file of the IR should be given as an argument when running the script.\n",
    "\n",
    "To test your implementation, you should be able to successfully load each of the three IR\n",
    "model files we have been working with throughout the course so far, which you can find in the\n",
    "`/home/workspace/models` directory.\n",
    "\n",
    "## Solution\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "The solution has been added to the feed_network.py file. Check the \"TODO\" sections in the file. <br/>\n",
    "\n",
    "**Note**: The model files are available under [L2-Leveraging-Pre-Trained-Model](https://github.com/frasheed-dev/Intel-Edge-AI-Fundamentals-with-OpenVINO/tree/main/excercise/L2-Leveraging-Pre-Trained-Models) excercise. <br/>\n",
    "\n",
    "**running solution:**\n",
    "<code>python feed_network.py -m [model_path]/human-pose-estimation-0001.xml</code>\n",
    "\n",
    "</div>\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3cd6d626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excercise 1: Solution Explanation video from the course\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"560\"\n",
       "            height=\"315\"\n",
       "            src=\"https://www.youtube.com/embed/jEmebNVBlc4\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x1c9e6507160>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Excercise 1: Solution Explanation video from the course\n",
    "print(\"Excercise 1: Solution Explanation video from the course\")\n",
    "IFrame(width=\"560\", height=\"315\", src=\"https://www.youtube.com/embed/jEmebNVBlc4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394437ff",
   "metadata": {},
   "source": [
    "# Excercise 2. Inference Requests\n",
    "\n",
    "Make sure to click the button below before you get started to source the correct environment.\n",
    "\n",
    "In the previous exercise, you loaded Intermediate Representations (IRs) into the Inference\n",
    "Engine. Now that we've covered some of the topics around requests, including the difference\n",
    "between synchronous and asynchronous requests, you'll add additional code to make\n",
    "inference requests to the Inference Engine.\n",
    "\n",
    "Given an `ExecutableNetwork` that is the IR loaded into the Inference Engine, your task is to:\n",
    "\n",
    "1. Perform a synchronous request\n",
    "2. Start an asynchronous request given an input image frame\n",
    "3. Wait for the asynchronous request to complete\n",
    "\n",
    "Note that we'll cover handling the results of the request shortly, so you don't need to worry\n",
    "about that just yet. This will get you practice with both types of requests with the Inference\n",
    "Engine.\n",
    "\n",
    "You will perform the above tasks within `inference.py`. This will take three arguments,\n",
    "one for the model, one for the test image, and the last for what type of inference request\n",
    "should be made.\n",
    "\n",
    "You can use `test.py` afterward to verify your code successfully makes inference requests.\n",
    "\n",
    "## Solution\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "The solution has been added to the inference.py file. Check the \"TODO\" sections in the file. <br/>\n",
    "\n",
    "**Note**: The model files and images are available under [L2-Leveraging-Pre-Trained-Model](https://github.com/frasheed-dev/Intel-Edge-AI-Fundamentals-with-OpenVINO/tree/main/excercise/L2-Leveraging-Pre-Trained-Models) excercise. <br/>\n",
    "\n",
    "**running solution:**\n",
    "<code>python feed_network.py -m [model_path]/human-pose-estimation-0001.xml</code>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f7ce3220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excercise 2: Solution Explanation video from the course\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"560\"\n",
       "            height=\"315\"\n",
       "            src=\"https://www.youtube.com/embed/QeBpEkkoZ74\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x1c9e68850d0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Excercise 2: Solution Explanation video from the course\n",
    "print(\"Excercise 2: Solution Explanation video from the course\")\n",
    "IFrame(width=\"560\", height=\"315\", src=\"https://www.youtube.com/embed/QeBpEkkoZ74\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327d9ac1",
   "metadata": {},
   "source": [
    "# Excercise 3. Convert an ONNX Model\n",
    "\n",
    "### Exercise Instructions\n",
    "\n",
    "In this exercise, you'll convert an ONNX Model into an Intermediate Representation using the \n",
    "Model Optimizer. You can find the related documentation [here](https://docs.openvino.ai/latest/openvino_docs_MO_DG_prepare_model_convert_model_Convert_Model_From_ONNX.html).\n",
    "\n",
    "For this exercise, first download the bvlc_alexnet model from [here](https://s3.amazonaws.com/download.onnx/models/opset_8/bvlc_alexnet.tar.gz). Use the `tar -xvf` command with the downloaded file to unpack it.\n",
    "\n",
    "Follow the documentation above and feed in the ONNX model to the Model Optimizer.\n",
    "\n",
    "If the conversion is successful, the terminal should let you know that it generated an IR model.\n",
    "The locations of the `.xml` and `.bin` files, as well as execution time of the Model Optimizer,\n",
    "will also be output.\n",
    "\n",
    "### PyTorch models\n",
    "\n",
    "Note that we will only cover converting directly from an ONNX model here. If you are interested\n",
    "in converting a PyTorch model using ONNX for use with OpenVINO, check out this [link](https://michhar.github.io/convert-pytorch-onnx/) for the steps to do so. From there, you can follow the steps in the rest\n",
    "of this exercise once you have an ONNX model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3fb2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Excercise 3. Solution\n",
    "\n",
    "#first download the model\n",
    "!wget https://s3.amazonaws.com/download.onnx/models/opset_8/bvlc_alexnet.tar.gz\n",
    "\n",
    "#extract model\n",
    "!tar -xvf bvlc_alexnet.tar.gz\n",
    "\n",
    "#conver model\n",
    "#syntax (from documentation): python3 mo.py --input_model <INPUT_MODEL>.onnx --output_dir <OUTPUT_MODEL_DIR>\n",
    "!python \\\n",
    "/opt/intel/openvino/deployment_tools/model_optimizer/mo.py \\\n",
    "--input_model bvlc_alexnet/model.onnx"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
