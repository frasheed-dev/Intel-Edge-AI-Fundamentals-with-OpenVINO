{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d43b99c",
   "metadata": {},
   "source": [
    "# Lesson 3: The Model Optimizer\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Note:</b> Intel OpenVINO toolkit should be installed and sourced to run the given code\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2ec8f4",
   "metadata": {},
   "source": [
    "# Excercise 1. Convert a TensorFlow Model\n",
    "\n",
    "In this exercise, you'll convert a TensorFlow Model from the Object Detection Model Zoo\n",
    "into an Intermediate Representation using the Model Optimizer.\n",
    "\n",
    "As noted in the related [documentation](https://docs.openvino.ai/latest/openvino_docs_MO_DG_prepare_model_convert_model_Convert_Model_From_TensorFlow.html), \n",
    "there is a difference in method when using a frozen graph vs. an unfrozen graph. Since\n",
    "freezing a graph is a TensorFlow-based function and not one specific to OpenVINO itself,\n",
    "in this exercise, you will only need to work with a frozen graph. However, I encourage you to\n",
    "try to freeze and load an unfrozen model on your own as well.\n",
    "\n",
    "For this exercise, first download the SSD MobileNet V2 COCO model from [here](http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v2_coco_2018_03_29.tar.gz). Use the `tar -xvf` \n",
    "command with the downloaded file to unpack it.\n",
    "\n",
    "From there, find the **Convert a TensorFlow\\* Model** header in the documentation, and\n",
    "feed in the downloaded SSD MobileNet V2 COCO model's `.pb` file. \n",
    "\n",
    "If the conversion is successful, the terminal should let you know that it generated an IR model.\n",
    "The locations of the `.xml` and `.bin` files, as well as execution time of the Model Optimizer,\n",
    "will also be output.\n",
    "\n",
    "**Note**: Converting the TF model will take a little over one minute in the workspace.\n",
    "\n",
    "### Hints & Troubleshooting\n",
    "\n",
    "Make sure to pay attention to the note in this section regarding the \n",
    "`--reverse_input_channels` argument. \n",
    "If you are unsure about this argument, you can read more [here](https://docs.openvinotoolkit.org/2019_R3/_docs_MO_DG_prepare_model_convert_model_Converting_Model_General.html#when_to_reverse_input_channels).\n",
    "\n",
    "There is additional documentation specific to converting models from TensorFlow's Object\n",
    "Detection Zoo [here](https://docs.openvinotoolkit.org/2019_R3/_docs_MO_DG_prepare_model_convert_model_tf_specific_Convert_Object_Detection_API_Models.html).\n",
    "You will likely need both the `--tensorflow_use_custom_operations_config` and\n",
    "`--tensorflow_object_detection_api_pipeline_config` arguments fed with their \n",
    "related files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7d33e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Excercise 1. Solution\n",
    "\n",
    "#first download the model\n",
    "!wget http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v2_coco_2018_03_29.tar.gz\n",
    "\n",
    "#extract model\n",
    "!tar -xvf ssd_mobilenet_v2_coco_2018_03_29.tar.gz\n",
    "\n",
    "#conver model\n",
    "#syntax (from documentation): python3 mo_tf.py --input_model <INPUT_MODEL>.pb --output_dir <OUTPUT_MODEL_DIR> \n",
    "#Note: In OpenVINOâ„¢ Toolkit 2020R1 (and likely future updates), --tensorflow_use_custom_operations_config (from 2019R3) was re-named to --transformations_config.\n",
    "!python \\\n",
    "/opt/intel/openvino/deployment_tools/model_optimizer/mo.py \\\n",
    "--input_model ssd_mobilenet_v2_coco_2018_03_29/frozen_inference_graph.pb \\\n",
    "--tensorflow_object_detection_api_pipeline_config ssd_mobilenet_v2_coco_2018_03_29/pipeline.config \\\n",
    "--reverse_input_channels \\\n",
    "--tensorflow_use_custom_operations_config /opt/intel/openvino/deployment_tools/model_optimizer/extensions/front/tf/ssd_v2_support.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394437ff",
   "metadata": {},
   "source": [
    "# Excercise 2. Convert a Caffe Model\n",
    "\n",
    "In this exercise, you'll convert a Caffe Model into an Intermediate Representation using the \n",
    "Model Optimizer. You can find the related documentation [here](https://docs.openvino.ai/latest/openvino_docs_MO_DG_prepare_model_convert_model_Convert_Model_From_Caffe.html).\n",
    "\n",
    "For this exercise, first download the SqueezeNet V1.1 model by cloning [this repository](https://github.com/DeepScale/SqueezeNet). \n",
    "\n",
    "Follow the documentation above and feed in the Caffe model to the Model Optimizer.\n",
    "\n",
    "If the conversion is successful, the terminal should let you know that it generated an IR model.\n",
    "The locations of the `.xml` and `.bin` files, as well as execution time of the Model Optimizer,\n",
    "will also be output.\n",
    "\n",
    "### Hints & Troubleshooting\n",
    "\n",
    "You will need to specify `--input_proto` if the `.prototxt` file is not named the same as the model.\n",
    "\n",
    "There is an important note in the documentation after the section **Supported Topologies** \n",
    "regarding Caffe models trained on ImageNet. If you notice poor performance in inference, you\n",
    "may need to specify mean and scale values in your arguments.\n",
    "\n",
    "```\n",
    "python /opt/intel/openvino/deployment_tools/model_optimizer/mo.py --input_model squeezenet_v1.1.caffemodel --input_proto deploy.prototxt\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168b47c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Excercise 2. Solution\n",
    "\n",
    "#clone the reposiroy where the model is\n",
    "!git clone https://github.com/DeepScale/SqueezeNet\n",
    "\n",
    "#model path is: ./SqueezeNet/SqueezeNet_v1.1/\n",
    "\n",
    "#conver model\n",
    "#syntax (from documentation): python3 mo.py --input_model <INPUT_MODEL>.caffemodel --output_dir <OUTPUT_MODEL_DIR>\n",
    "!python \\\n",
    "/opt/intel/openvino/deployment_tools/model_optimizer/mo.py \\\n",
    "--input_model ./SqueezeNet/SqueezeNet_v1.1/squeezenet_v1.1.caffemodel \\\n",
    "--input_proto ./SqueezeNet/SqueezeNet_v1.1/deploy.prototxt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327d9ac1",
   "metadata": {},
   "source": [
    "# Excercise 3. Convert an ONNX Model\n",
    "\n",
    "### Exercise Instructions\n",
    "\n",
    "In this exercise, you'll convert an ONNX Model into an Intermediate Representation using the \n",
    "Model Optimizer. You can find the related documentation [here](https://docs.openvino.ai/latest/openvino_docs_MO_DG_prepare_model_convert_model_Convert_Model_From_ONNX.html).\n",
    "\n",
    "For this exercise, first download the bvlc_alexnet model from [here](https://s3.amazonaws.com/download.onnx/models/opset_8/bvlc_alexnet.tar.gz). Use the `tar -xvf` command with the downloaded file to unpack it.\n",
    "\n",
    "Follow the documentation above and feed in the ONNX model to the Model Optimizer.\n",
    "\n",
    "If the conversion is successful, the terminal should let you know that it generated an IR model.\n",
    "The locations of the `.xml` and `.bin` files, as well as execution time of the Model Optimizer,\n",
    "will also be output.\n",
    "\n",
    "### PyTorch models\n",
    "\n",
    "Note that we will only cover converting directly from an ONNX model here. If you are interested\n",
    "in converting a PyTorch model using ONNX for use with OpenVINO, check out this [link](https://michhar.github.io/convert-pytorch-onnx/) for the steps to do so. From there, you can follow the steps in the rest\n",
    "of this exercise once you have an ONNX model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3fb2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Excercise 3. Solution\n",
    "\n",
    "#first download the model\n",
    "!wget https://s3.amazonaws.com/download.onnx/models/opset_8/bvlc_alexnet.tar.gz\n",
    "\n",
    "#extract model\n",
    "!tar -xvf bvlc_alexnet.tar.gz\n",
    "\n",
    "#conver model\n",
    "#syntax (from documentation): python3 mo.py --input_model <INPUT_MODEL>.onnx --output_dir <OUTPUT_MODEL_DIR>\n",
    "!python \\\n",
    "/opt/intel/openvino/deployment_tools/model_optimizer/mo.py \\\n",
    "--input_model bvlc_alexnet/model.onnx"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
