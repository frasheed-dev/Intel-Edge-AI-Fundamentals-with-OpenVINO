{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d43b99c",
   "metadata": {},
   "source": [
    "# Lesson 2: Leveraging-Pre-Trained-Models\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Note:</b> Intel OpenVINO toolkit should be installed and sourced to run the given code\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa128677",
   "metadata": {},
   "source": [
    "# Exercise 1. Loading Pre-Trained Models\n",
    "\n",
    "In this exercise, you'll work to download and load a few of the pre-trained models available \n",
    "in the OpenVINO toolkit.\n",
    "\n",
    "First, you can navigate to the [Pre-Trained Models list](https://software.intel.com/en-us/openvino-toolkit/documentation/pretrained-models) in a separate window or tab, as well as the page that gives all of the model names [here](https://docs.openvinotoolkit.org/2019_R3/_models_intel_index.html).\n",
    "\n",
    "Your task here is to download the below three pre-trained models using the Model Downloader tool, as detailed on the same page as the different model names. Note that you *do not need to download all of the available pre-trained models* - doing so would cause your workspace to crash, as the workspace will limit you to 3 GB of downloaded models.\n",
    "\n",
    "### Task 1 - Find the Right Models\n",
    "Using the [Pre-Trained Model list](https://software.intel.com/en-us/openvino-toolkit/documentation/pretrained-models), determine which models could accomplish the following tasks (there may be some room here in determining which model to download):\n",
    "- Human Pose Estimation (**solution:** [human-pose-estimation-0001](https://docs.openvinotoolkit.org/latest/_models_intel_human_pose_estimation_0001_description_human_pose_estimation_0001.html))\n",
    "- Text Detection solution: (**solution:** [text-detection-0004](http://docs.openvinotoolkit.org/latest/_models_intel_text_detection_0004_description_text_detection_0004.html))\n",
    "- Determining Car Type & Color (**solution:** [vehicle-attributes-recognition-barrier-0039](https://docs.openvinotoolkit.org/latest/_models_intel_vehicle_attributes_recognition_barrier_0039_description_vehicle_attributes_recognition_barrier_0039.html))\n",
    "\n",
    "### Task 2 - Download the Models\n",
    "Once you have determined which model best relates to the above tasks, use the Model Downloader tool to download them into the workspace for the following precision levels:\n",
    "- Human Pose Estimation: All precision levels\n",
    "- Text Detection: FP16 only\n",
    "- Determining Car Type & Color: INT8 only\n",
    "\n",
    "**Note**: When downloading the models in the workspace, add the `-o` argument (along with any other necessary arguments) with `/home/workspace` as the output directory. The default download directory will not allow the files to be written there within the workspace, as it is a read-only directory.\n",
    "\n",
    "#### Task 2 - Solution\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>First navigate to model downloaded directory:</b> <br/>\n",
    "&nbsp; &nbsp; <code>cd /opt/intel/openvino/deployment_tools/open_model_zoo/tools/downloader </code>.\n",
    "</div>\n",
    "\n",
    "- Human Pose Estimation: <br/>\n",
    "&nbsp; &nbsp; <code>python downloader.py --name human-pose-estimation-0001 -o /home/workspace</code>\n",
    "- Text Detection: <br/>\n",
    "&nbsp; &nbsp; <code>python downloader.py --name text-detection-0004 --precisions FP16 -o /home/workspace</code>\n",
    "- Determining Car Type & Color: <br/>\n",
    "&nbsp; &nbsp; <code>python downloader.py --name vehicle-attributes-recognition-barrier-0039 --precisions INT8 -o /home/workspace\n",
    "</code>\n",
    "\n",
    "### Task 3 - Verify the Downloads\n",
    "You can verify the download of these models by navigating to: `/home/workspace/intel` (if you followed the above note), and checking whether a directory was created for each of the three models, with included subdirectories for each precision, with respective `.bin` and `.xml` for each model.\n",
    "\n",
    "**Hint**: Use the `-h` command with the Model Downloader tool if you need to check out the possible arguments to include when downloading specific models and precisions.\n",
    "\n",
    "#### Task 3 - Solution\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Check the directory where models were downloaded:</b> <br/>\n",
    "&nbsp; &nbsp; <code>tree /home/workspace/</code>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19efffa3",
   "metadata": {},
   "source": [
    "# Exercise 2. Pre-processing Inputs\n",
    "\n",
    "Now that we have a few pre-trained models downloaded, it's time to preprocess the inputs\n",
    "to match what each of the models expects as their input. We'll use the same models as before\n",
    "as a basis for determining the preprocessing necessary for each input file.\n",
    "\n",
    "As a reminder, our three models are:\n",
    "- Human Pose Estimation: [human-pose-estimation-0001](https://docs.openvinotoolkit.org/2019_R3/_models_intel_human_pose_estimation_0001_description_human_pose_estimation_0001.html)\n",
    "- Text Detection: [text-detection-0004](http://docs.openvinotoolkit.org/2019_R3/_models_intel_text_detection_0004_description_text_detection_0004.html)\n",
    "- Determining Car Type & Color: [vehicle-attributes-recognition-barrier-0039](https://docs.openvinotoolkit.org/2019_R3/_models_intel_vehicle_attributes_recognition_barrier_0039_description_vehicle_attributes_recognition_barrier_0039.html)\n",
    "\n",
    "**Note:** For ease of use, these models have been added into the `/home/workspace/models`\n",
    "directory. For example, if you need to use the Text Detection model, you could find it at:\n",
    "\n",
    "```bash\n",
    "/home/workspace/models/text_detection_0004.xml\n",
    "```\n",
    "\n",
    "Each link above contains the documentation for the related model. In our case, we want to \n",
    "focus on the **Inputs** section of the page, wherein important information regarding the input\n",
    "shape, order of the shape (such as color channel first or last), and the order of the color\n",
    "channels, is included.\n",
    "\n",
    "Your task is to fill out the code in three functions within `preprocess_inputs.py`, one for \n",
    "each of the three models. We have also included a potential sample image for each of the \n",
    "three models, that will be used with `test.py` to check whether the\n",
    "input for each model has been adjusted as expected for proper model input.\n",
    "\n",
    "Note that each image is **currently loaded as BGR with H, W, C order** in the `test.py` file,\n",
    "so any necessary preprocessing to change that should occur in your three work files. \n",
    "Note that **BGR** order is used, as the OpenCV function we use to read images loads as\n",
    "BGR, and not RGB.\n",
    "\n",
    "When finished, you should be able to run the `test.py` file and pass all three tests.\n",
    "\n",
    "## Solution:\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "Since the cv.imread function reads the images as BGR with H(eight),W(idth),C(hannels) order,\n",
    "we have to: <br/>\n",
    "(1) resize-image (based on the model requirement) <br/>\n",
    "(2) process images as RGB <br/>\n",
    "(3) re-shape image for as B(atch), C(hannel), width, heightm B=1, C=3 <br/>\n",
    "    \n",
    "The \"proprocssing\" function has been added to the 'preprocess_inputs.py', run <code>python test.py</code> to verify.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c544e9",
   "metadata": {},
   "source": [
    "# Exercise 3. Deploy Your First Edge App\n",
    "\n",
    "So far, you've downloaded some pre-trained models, handled their inputs, and learned how\n",
    "to handle outputs. In this exercise, you'll implement the handling of the outputs of our three\n",
    "models from before, and get to see inference actually performed by adding these models\n",
    "to some example edge applications. \n",
    "\n",
    "There's a lot of code still involved behind the scenes here. With the Pre-Trained Models \n",
    "available with the OpenVINO toolkit, you don't need to worry about the Model Optimizer, but\n",
    "there is still work done to load the model into the Inference Engine. We won't learn about \n",
    "this code until later, so in this case, you'll just need to call your functions to handle the input\n",
    "and output of the model within the app.\n",
    "\n",
    "If you do want a sneak preview of some of the code that interfaces with the Inference Engine,\n",
    "you can check it out in `inference.py`. You'll work out of the `handle_models.py` file, as \n",
    "well as adding functions calls within the edge app in `app.py`.\n",
    "\n",
    "## TODOs\n",
    "\n",
    "In `handle_models.py`, you will need to implement `handle_pose`, `handle_text`, and\n",
    "`handle_car`.\n",
    "\n",
    "In `app.py`, first, you'll need to use the input shape of the network to call the `preprocessing`\n",
    "function. Then, you need to call `handle_output` with the appropriate model argument \n",
    "in order to get the right handling function. With that function, you can then feed the output\n",
    "of the inference request in in order to extract the output. \n",
    "\n",
    "Note that there is some additional post-processing done for you in `create_output_image`\n",
    "within `app.py` to help display the output back onto the input image.\n",
    "\n",
    "## Testing the apps\n",
    "\n",
    "To test your implementations, you can use `app.py` to run each edge application, with\n",
    "the following arguments:\n",
    "- `-t`: The model type,  which should be one of `\"POSE\"`, `\"TEXT\"`, or `\"CAR_META\"`\n",
    "- `-m`: The location of the model .xml file\n",
    "- `-i`: The location of the input image used for testing\n",
    "- `-c`: A CPU extension file, if applicable. See below for what this is for the workspace.\n",
    "The results of your output will be saved down for viewing in the `outputs` directory.\n",
    "\n",
    "As an example, here is an example of running the app with related arguments:\n",
    "\n",
    "```\n",
    "python app.py -i \"images/blue-car.jpg\" -t \"CAR_META\" -m \"/home/workspace/models/vehicle-attributes-recognition-barrier-0039.xml\" -c \"/opt/intel/openvino/deployment_tools/inference_engine/lib/intel64/libcpu_extension_sse4.so\"\n",
    "```\n",
    "\n",
    "## Model Documentation\n",
    "\n",
    "Once again, here are the links to the models, so you can use the **Output** section to help\n",
    "you get started (there are additional comments in the code to assist):\n",
    "\n",
    "- Human Pose Estimation: [human-pose-estimation-0001](https://docs.openvino.ai/latest/omz_models_model_human_pose_estimation_0001.html)\n",
    "- Text Detection: [text-detection-0004](https://docs.openvino.ai/latest/omz_models_model_text_detection_0004.html)\n",
    "- Determining Car Type & Color: [vehicle-attributes-recognition-barrier-0039](https://docs.openvino.ai/2019_R1/_vehicle_attributes_recognition_barrier_0039_description_vehicle_attributes_recognition_barrier_0039.html)\n",
    "\n",
    "\n",
    "## Solution:\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "The solution has been added to the app.py and handle_models.py. Check the \"TODO\" section in both files <br/>\n",
    "    \n",
    "**Example code**: <br/>\n",
    "<code>python app.py -i \"images/blue-car.jpg\" -t \"CAR_META\" -m \"/home/workspace/models/vehicle-attributes-recognition-barrier-0039.xml\" -c \"/opt/intel/openvino/deployment_tools/inference_engine/lib/intel64/libcpu_extension_sse4.so\"</code>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc8938f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
