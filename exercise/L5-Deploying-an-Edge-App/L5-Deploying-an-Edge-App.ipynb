{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d43b99c",
   "metadata": {},
   "source": [
    "# Lesson 5: Deploying an Edge App\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Note:</b> Intel OpenVINO toolkit should be installed and sourced to run the given code\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c577625",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import IFrame #to show YT videos on notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2ec8f4",
   "metadata": {},
   "source": [
    "# Exercise 1. Handling Input Streams\n",
    "\n",
    "It's time to really get in the think of things for running your app at the edge. Being able to\n",
    "appropriately handle an input stream is a big part of having a working AI or computer vision\n",
    "application. \n",
    "\n",
    "In your case, you will be implementing a function that can handle camera, video or webcam\n",
    "data as input. While unfortunately the classroom workspace won't allow for webcam usage,\n",
    "you can also try that portion of your code out on your local machine if you have a webcam\n",
    "available.\n",
    "\n",
    "As such, the tests here will focus on using a camera image or a video file. You will not need to\n",
    "perform any inference on the input frames, but you will need to do a few other image\n",
    "processing techniques to show you have some of the basics of OpenCV down.\n",
    "\n",
    "Your tasks are to:\n",
    "\n",
    "1. Implement a function that can handle camera image, video file or webcam inputs\n",
    "2. Use `cv2.VideoCapture()` and open the capture stream\n",
    "3. Re-size the frame to 100x100\n",
    "4. Add Canny Edge Detection to the frame with min & max values of 100 and 200, respectively\n",
    "5. Save down the image or video output\n",
    "6. Close the stream and any windows at the end of the application\n",
    "\n",
    "You won't be able to test a webcam input in the workspace unfortunately, but you can use\n",
    "the included video and test image to test your implementations.\n",
    "\n",
    "## Solution\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "The solution has been added to the app.py file. Check the \"TODO\" sections in the file. <br/>\n",
    "\n",
    "**Note**: The model, images and video files are available under [L2-Leveraging-Pre-Trained-Model](https://github.com/frasheed-dev/Intel-Edge-AI-Fundamentals-with-OpenVINO/tree/main/exercise/L2-Leveraging-Pre-Trained-Models) and [L4-The-Inference-Engine](https://github.com/frasheed-dev/Intel-Edge-AI-Fundamentals-with-OpenVINO/tree/main/exercise/L4-The-Inference-Engine) exercises. <br/>\n",
    "\n",
    "**Running solution:**\n",
    "    \n",
    "**Run for image**\n",
    "<code>python app.py -i blue-car.jpg</code>\n",
    "    \n",
    "**Run for video**\n",
    "<code>python app.py -i test_video.mp4\n",
    "</code>\n",
    "\n",
    "\n",
    "</div>\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "201668cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exercise 1: Solution Explanation video from the course\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"560\"\n",
       "            height=\"315\"\n",
       "            src=\"https://www.youtube.com/embed/p6Hd3dnf-LY\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x1c934bb4580>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exercise 1: Solution Explanation video from the course\n",
    "print(\"Exercise 1: Solution Explanation video from the course\")\n",
    "IFrame(width=\"560\", height=\"315\", src=\"https://www.youtube.com/embed/p6Hd3dnf-LY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394437ff",
   "metadata": {},
   "source": [
    "# Excercise 2. Processing Model Outputs\n",
    "\n",
    "Let's say you have a cat and two dogs at your house. \n",
    "\n",
    "If both dogs are in a room together, they are best buds, and everything is going well.\n",
    "\n",
    "If the cat and dog #1 are in a room together, they are also good friends, and everything is fine.\n",
    "\n",
    "However, if the cat and dog #2 are in a room together, they don't get along, and you may need\n",
    "to either pull them apart, or at least play a pre-recorded message from your smart speaker\n",
    "to tell them to cut it out.\n",
    "\n",
    "In this exercise, you'll receive a video where some combination or the cat and dogs may be\n",
    "in view. You also will have an IR that is able to determine which of these, if any, are on screen.\n",
    "\n",
    "While the best model for this is likely an object detection model that can identify different\n",
    "breeds, I have provided you with a very basic (and overfit) model that will return three classes,\n",
    "one for one or less pets on screen, one for the bad combination of the cat and dog #2, and\n",
    "one for the fine combination of the cat and dog #1. This is within the exercise directory - `model.xml`.\n",
    "\n",
    "It is up to you to add code that will print to the terminal anytime the bad combination of the \n",
    "cat and dog #2 are detected together. **Note**: It's important to consider whether you really\n",
    "want to output a warning *every single time* both pets are on-screen - is your warning helpful\n",
    "if it re-starts every 30th of a second, with a video at 30 fps?\n",
    "\n",
    "## Solution\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "The solution has been added to the app_ex2.py file. Check the \"TODO\" sections in the file. <br/>\n",
    "\n",
    "**Note**: The model files (.bin and .xml) and input video uploaded in this exercise folder. <br/>\n",
    "\n",
    "**Running solution:**\n",
    "<code>python app_ex2.py -m model.xml</code>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6758d15f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excercise 2: Solution Explanation video from the course\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"560\"\n",
       "            height=\"315\"\n",
       "            src=\"https://www.youtube.com/embed/s35d7IvQliE\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x1c934bd5490>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exercise 2: Solution Explanation video from the course\n",
    "print(\"Excercise 2: Solution Explanation video from the course\")\n",
    "IFrame(width=\"560\", height=\"315\", src=\"https://www.youtube.com/embed/s35d7IvQliE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327d9ac1",
   "metadata": {},
   "source": [
    "# Excercise 3. Server Communications\n",
    "\n",
    "In this exercise, you will practice showing off your new server communication skills\n",
    "for sending statistics over MQTT and images with FFMPEG.\n",
    "\n",
    "The application itself is already built and able to perform inference, and a node server is set\n",
    "up for you to use. The main node server is already fully ready to receive communications from\n",
    "MQTT and FFMPEG. The MQTT node server is fully configured as well. Lastly, the ffserver is \n",
    "already configured for FFMPEG too.\n",
    "\n",
    "The current application simply performs inference on a frame, gathers some statistics, and then \n",
    "continues onward to the next frame. \n",
    "\n",
    "## Tasks\n",
    "\n",
    "Your tasks are to:\n",
    "\n",
    "- Add any code for MQTT to the project so that the node server receives the calculated stats\n",
    "  - This includes importing the relevant Python library\n",
    "  - Setting IP address and port\n",
    "  - Connecting to the MQTT client\n",
    "  - Publishing the calculated statistics to the client\n",
    "- Send the output frame (**not** the input image, but the processed output) to the ffserver\n",
    "\n",
    "## Additional Information\n",
    "\n",
    "Note: Since you are given the MQTT Broker Server and Node Server for the UI, you need \n",
    "certain information to correctly configure, publish and subscribe with MQTT.\n",
    "- The MQTT port to use is 3001 - the classroom workspace only allows ports 3000-3009\n",
    "- The topics that the UI Node Server is listening to are \"class\" and \"speedometer\"\n",
    "- The Node Server will attempt to extract information from any JSON received from the MQTT server with the keys \"class_names\" and \"speed\"\n",
    "\n",
    "## Running the App\n",
    "\n",
    "First, get the MQTT broker and UI installed.\n",
    "\n",
    "- `cd webservice/server`\n",
    "- `npm install`\n",
    "- When complete, `cd ../ui`\n",
    "- And again, `npm install`\n",
    "\n",
    "You will need *four* separate terminal windows open in order to see the results. The steps\n",
    "below should be done in a different terminal based on number. You can open a new terminal\n",
    "in the workspace in the upper left (File>>New>>Terminal).\n",
    "\n",
    "1. Get the MQTT broker installed and running.\n",
    "  - `cd webservice/server/node-server`\n",
    "  - `node ./server.js`\n",
    "  - You should see a message that `Mosca server started.`.\n",
    "2. Get the UI Node Server running.\n",
    "  - `cd webservice/ui`\n",
    "  - `npm run dev`\n",
    "  - After a few seconds, you should see `webpack: Compiled successfully.`\n",
    "3. Start the ffserver\n",
    "  - `sudo ffserver -f ./ffmpeg/server.conf`\n",
    "4. Start the actual application. \n",
    "  - First, you need to source the environment for OpenVINO *in the new terminal*:\n",
    "    - `source /opt/intel/openvino/bin/setupvars.sh -pyver 3.5`\n",
    "  - To run the app, I'll give you two items to pipe in with `ffmpeg` here, with the rest up to you:\n",
    "    - `-video_size 1280x720`\n",
    "    - `-i - http://0.0.0.0:3004/fac.ffm`\n",
    "\n",
    "Your app should begin running, and you should also see the MQTT broker server noting\n",
    "information getting published.\n",
    "\n",
    "\n",
    "## Solution\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "The solution has been added to the app_ex3.py and inference_ex3.py file. Check the \"TODO\" sections in the files. <br/>\n",
    "\n",
    "**Note**: The model files and test video are available under [L2-Leveraging-Pre-Trained-Model](https://github.com/frasheed-dev/Intel-Edge-AI-Fundamentals-with-OpenVINO/tree/main/excercise/L2-Leveraging-Pre-Trained-Models) and [L4-The-Inference-Engine](https://github.com/frasheed-dev/Intel-Edge-AI-Fundamentals-with-OpenVINO/tree/main/exercise/L4-The-Inference-Engine)exercise. <br/>\n",
    "\n",
    "**Running solution:**\n",
    "To run the app itself, with the UI server, MQTT server, and FFmpeg server also running, do:\n",
    "\n",
    "<code>python app_ex3.py | ffmpeg -v warning -f rawvideo \n",
    "-pixel_format bgr24 -video_size 1280x720 -framerate 24 -i - http://0.0.0.0:3004/fac.ffm</code>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4761f588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excercise 3: Solution Explanation video from the course\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"560\"\n",
       "            height=\"315\"\n",
       "            src=\"https://www.youtube.com/embed/c2cNJgrvHmg\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x1c934bd75b0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exercise 3: Solution Explanation video from the course\n",
    "print(\"Excercise 3: Solution Explanation video from the course\")\n",
    "IFrame(width=\"560\", height=\"315\", src=\"https://www.youtube.com/embed/c2cNJgrvHmg\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
