{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d43b99c",
   "metadata": {},
   "source": [
    "# Lesson 4: The Inference Engine\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Note:</b> Intel OpenVINO toolkit should be installed and sourced to run the given code\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c577625",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import IFrame #to show YT videos on notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2ec8f4",
   "metadata": {},
   "source": [
    "# Exercise 1. Feed an IR to the Inference Engine\n",
    "\n",
    "Make sure to click the button below before you get started to source the correct environment.\n",
    "\n",
    "\n",
    "Earlier in the course, you were focused on working with the Intermediate Representation (IR)\n",
    "models themselves, while mostly glossing over the use of the actual Inference Engine with\n",
    "the model.\n",
    "\n",
    "Here, you'll import the Python wrapper for the Inference Engine (IE), and practice using \n",
    "different IRs with it. You will first add each IR as an `IENetwork`, and check whether the layers \n",
    "of that network are supported by the classroom CPU.\n",
    "\n",
    "Since the classroom workspace is using an Intel CPU, you will also need to add a CPU\n",
    "extension to the `IECore`.\n",
    "\n",
    "Once you have verified all layers are supported (when the CPU extension is added),\n",
    "you will load the given model into the Inference Engine.\n",
    "\n",
    "Note that the `.xml` file of the IR should be given as an argument when running the script.\n",
    "\n",
    "To test your implementation, you should be able to successfully load each of the three IR\n",
    "model files we have been working with throughout the course so far, which you can find in the\n",
    "`/home/workspace/models` directory.\n",
    "\n",
    "## Solution\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "The solution has been added to the feed_network.py file. Check the \"TODO\" sections in the file. <br/>\n",
    "\n",
    "**Note**: The model files are available under [L2-Leveraging-Pre-Trained-Model](https://github.com/frasheed-dev/Intel-Edge-AI-Fundamentals-with-OpenVINO/tree/main/
    /L2-Leveraging-Pre-Trained-Models) exercise. <br/>\n",
    "\n",
    "**Running solution:**\n",
    "<code>python feed_network.py -m [model_path]/human-pose-estimation-0001.xml</code>\n",
    "\n",
    "</div>\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "201668cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exercise 1: Solution Explanation video from the course\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"560\"\n",
       "            height=\"315\"\n",
       "            src=\"https://www.youtube.com/embed/jEmebNVBlc4\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x1c9e6507160>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exercise 1: Solution Explanation video from the course\n",
    "print(\"Exercise 1: Solution Explanation video from the course\")\n",
    "IFrame(width=\"560\", height=\"315\", src=\"https://www.youtube.com/embed/jEmebNVBlc4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394437ff",
   "metadata": {},
   "source": [
    "# Exercise 2. Inference Requests\n",
    "\n",
    "In the previous exercise, you loaded Intermediate Representations (IRs) into the Inference\n",
    "Engine. Now that we've covered some of the topics around requests, including the difference\n",
    "between synchronous and asynchronous requests, you'll add additional code to make\n",
    "inference requests to the Inference Engine.\n",
    "\n",
    "Given an `ExecutableNetwork` that is the IR loaded into the Inference Engine, your task is to:\n",
    "\n",
    "1. Perform a synchronous request\n",
    "2. Start an asynchronous request given an input image frame\n",
    "3. Wait for the asynchronous request to complete\n",
    "\n",
    "Note that we'll cover handling the results of the request shortly, so you don't need to worry\n",
    "about that just yet. This will get you practice with both types of requests with the Inference\n",
    "Engine.\n",
    "\n",
    "You will perform the above tasks within `inference.py`. This will take three arguments,\n",
    "one for the model, one for the test image, and the last for what type of inference request\n",
    "should be made.\n",
    "\n",
    "You can use `test.py` afterward to verify your code successfully makes inference requests.\n",
    "\n",
    "## Solution\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "The solution has been added to the inference.py file. Check the \"TODO\" sections in the file. <br/>\n",
    "\n",
    "**Note**: The model files and images are available under [L2-Leveraging-Pre-Trained-Model](https://github.com/frasheed-dev/Intel-Edge-AI-Fundamentals-with-OpenVINO/tree/main/exercise/L2-Leveraging-Pre-Trained-Models) exercise. <br/>\n",
    "\n",
    "**Running solution:**\n",
    "<code>python test.py</code>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6758d15f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exercise 2: Solution Explanation video from the course\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"560\"\n",
       "            height=\"315\"\n",
       "            src=\"https://www.youtube.com/embed/QeBpEkkoZ74\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x1c9e68850d0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exercise 2: Solution Explanation video from the course\n",
    "print(\"Exercise 2: Solution Explanation video from the course\")\n",
    "IFrame(width=\"560\", height=\"315\", src=\"https://www.youtube.com/embed/QeBpEkkoZ74\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327d9ac1",
   "metadata": {},
   "source": [
    "# Exercise 3. Integrate the Inference Engine in An Edge App\n",
    "\n",
    "You've come a long way from the first lesson where most of the code for working with\n",
    "the OpenVINO toolkit was happening in the background. You worked with pre-trained models,\n",
    "moved up to converting any trained model to an Intermediate Representation with the\n",
    "Model Optimizer, and even got the model loaded into the Inference Engine and began making\n",
    "inference requests.\n",
    "\n",
    "In this final exercise of this lesson, you'll close off the OpenVINO workflow by extracting\n",
    "the results of the inference request, and then integrating the Inference Engine into an existing\n",
    "application. You'll still be given some of the overall application infrastructure, as more that of\n",
    "will come in the next lesson, but all of that is outside of OpenVINO itself.\n",
    "\n",
    "You will also add code allowing you to try out various confidence thresholds with the model,\n",
    "as well as changing the visual look of the output, like bounding box colors.\n",
    "\n",
    "Now, it's up to you which exact model you want to use here, although you are able to just\n",
    "re-use the model you converted with TensorFlow before for an easy bounding box dectector.\n",
    "\n",
    "Note that this application will run with a video instead of just images like we've done before.\n",
    "\n",
    "So, your tasks are to:\n",
    "\n",
    "1. Convert a bounding box model to an IR with the Model Optimizer.\n",
    "2. Pre-process the model as necessary.\n",
    "3. Use an async request to perform inference on each video frame.\n",
    "4. Extract the results from the inference request.\n",
    "5. Add code to make the requests and feed back the results within the application.\n",
    "6. Perform any necessary post-processing steps to get the bounding boxes.\n",
    "7. Add a command line argument to allow for different confidence thresholds for the model.\n",
    "8. Add a command line argument to allow for different bounding box colors for the output.\n",
    "9. Correctly utilize the command line arguments in #3 and #4 within the application.\n",
    "\n",
    "When you are done, feed your model to `app.py`, and it will generate `out.mp4`, which you\n",
    "can download and view. *Note that this app will take a little bit longer to run.* Also, if you need\n",
    "to re-run inference, delete the `out.mp4` file first.\n",
    "\n",
    "You only need to feed the model with `-m` before adding the customization; you should set\n",
    "defaults for any additional arguments you add for the color and confidence so that the user\n",
    "does not always need to specify them.\n",
    "\n",
    "<code>python app.py -m {your-model-path.xml}</code>\n",
    "\n",
    "## Solution\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "The solution has been added to the app.py and inference_ex3.py file. Check the \"TODO\" sections in the files. <br/>\n",
    "\n",
    "**Note**: The model files are available under [L2-Leveraging-Pre-Trained-Model](https://github.com/frasheed-dev/Intel-Edge-AI-Fundamentals-with-OpenVINO/tree/main/exercise/L2-Leveraging-Pre-Trained-Models) exercise while the input video (test_video.mp4) has been uploaded in this section. <br/>\n",
    "\n",
    "**Running solution:**\n",
    "There are two solution apps (app.py and app-custom.py) in this section. Assuming, you must have the weight file in the \"models\" directory.\n",
    "\n",
    "**Run without arguments**\n",
    "<code>python app.py -m models/frozen_inference_graph.xml</code>\n",
    "\n",
    "**Run with arguments**\n",
    "<code>python app-custom.py -m models/frozen_inference_graph.xml -ct 0.6 -c BLUE</code>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1e3fb2b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exercise 3: Solution Explanation video from the course\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"560\"\n",
       "            height=\"315\"\n",
       "            src=\"https://www.youtube.com/embed/BIdLJkDD5vM\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x1c9e65079d0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exercise 3: Solution Explanation video from the course\n",
    "print(\"Exercise 3: Solution Explanation video from the course\")\n",
    "IFrame(width=\"560\", height=\"315\", src=\"https://www.youtube.com/embed/BIdLJkDD5vM\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
